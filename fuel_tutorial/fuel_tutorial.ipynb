{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuel\n",
    "\n",
    "## History\n",
    "\n",
    "* Started as a part of **Blocks**, a framework for building and managing **Theano** graphs in the context of neural networks.\n",
    "* Became its own project when we realized it was distinct enough that it could be used by other frameworks too.\n",
    "\n",
    "## Goal\n",
    "\n",
    "*Simplify downloading, storing, iterating over and preprocessing data used to train machine learning models.*\n",
    "\n",
    "# Quick start\n",
    "\n",
    "We'll go over a quick example to see what Fuel is capable of.\n",
    "\n",
    "Let's start by creating some random data to act as features and targets. We'll pretend that we have eight 2x2 grayscale images separated into four classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "seed = 1234\n",
    "rng = numpy.random.RandomState(seed)\n",
    "features = rng.randint(256, size=(8, 2, 2))\n",
    "targets = rng.randint(4, size=(8, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to use Fuel to interface with this data, iterate over it in various ways and apply transformations to it on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Division of labor\n",
    "\n",
    "There are four basic tasks that Fuel needs to handle:\n",
    "\n",
    "* Interface with the data, be it on disk or in memory.\n",
    "* Decide which data points to visit, and in which order.\n",
    "* Iterate over the selected data points.\n",
    "* At each iteration step, apply some transformation to the selected data points.\n",
    "\n",
    "Each of those four tasks is delegated to a particular class of objects, which we'll be introducing in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets: interfacing with data\n",
    "\n",
    "The `Dataset` class is responsible for interfacing with the data and handling data access requests. Subclasses of `Dataset` specialize in certain types of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IterableDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest ``Dataset`` subclass is `IterableDataset`, which interfaces with iterable objects.\n",
    "\n",
    "It is created by passing a `dict` mapping source names to their associated data and, optionally, a `dict` mapping source names to tuples of axis labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuel.datasets import IterableDataset\n",
    "dataset = IterableDataset(\n",
    "    iterables={'features': features, 'targets': targets},\n",
    "    axis_labels={'features': ('batch', 'height', 'width'),\n",
    "                 'targets': ('batch', 'index')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ask the dataset what sources of data it provides by accessing its `sources` attribute. We can also know which axes correspond to what by accessing its `axis_labels` attribute. It also has a `num_examples` property telling us the number of examples it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Sources are {}.'.format(dataset.sources))\n",
    "print('Axis labels are {}.'.format(dataset.axis_labels))\n",
    "print('Dataset contains {} examples.'.format(dataset.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets themselves are stateless objects (as opposed to, say, an open file handle, or an iterator object). In order to request data from the dataset, we need to ask it to instantiate some stateful object with which it will interact. This is done through the `open` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state = dataset.open()\n",
    "print(state.__class__.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in `IterableDataset`'s case the state is an iterator object. We can now visit the examples this dataset contains using its `get_data` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(dataset.get_data(state=state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Note that the return order depends on the order of `dataset.sources`, which is nondeterministic if you use `dict` instances. In order to have deterministic behaviour, it is recommended that you use `OrderedDict` instances instead.)*\n",
    "\n",
    "Eventually, the iterator is depleted and it raises a `StopIteration` exception. We can iterate over the dataset again by requesting a fresh iterator through the dataset's `reset` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        dataset.get_data(state=state)\n",
    "    except StopIteration:\n",
    "        print('Iteration over')\n",
    "        break\n",
    "state = dataset.reset(state=state)\n",
    "print(dataset.get_data(state=state))\n",
    "dataset.close(state=state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IndexableDataset\n",
    "\n",
    "The `IterableDataset` implementation is pretty minimal. For instance, it only lets you iterate sequentially and examplewise over your data.\n",
    "\n",
    "If your data happens to be indexable (e.g. a list, or a numpy array), then `IndexableDataset` will let you do much more.\n",
    "\n",
    "We instantiate `IndexableDataset` just like `IterableDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuel.datasets import IndexableDataset\n",
    "from collections import OrderedDict\n",
    "\n",
    "dataset = IndexableDataset(\n",
    "    indexables=OrderedDict([('features', features), ('targets', targets)]),\n",
    "    axis_labels={'features': ('batch', 'height', 'width'), 'targets': ('batch', 'index')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of `IndexableDataset` over `IterableDataset` is that it allows random access of the data it contains. In order to do so, we need to pass an additional `request` argument to `get_data` in the form of a list of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state = dataset.open()\n",
    "print('State is {}'.format(state))\n",
    "print(dataset.get_data(state=state, request=[0, 1]))\n",
    "dataset.close(state=state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(See how `IndexableDataset` returns a `None` state: this is because there's no actual state to maintain in this case.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restricting sources\n",
    "\n",
    "In some cases (e.g. unsupervised learning), you might want to use a subset of the provided sources. This is achieved by passing a `sources` argument to the dataset constructor. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "restricted_dataset = IndexableDataset(\n",
    "    indexables=OrderedDict([('features', features), ('targets', targets)]),\n",
    "    axis_labels={'features': ('batch', 'height', 'width'), 'targets': ('batch', 'index')},\n",
    "    sources=('features',))\n",
    "\n",
    "state = restricted_dataset.open()\n",
    "print(restricted_dataset.get_data(state=state, request=[0, 1]))\n",
    "restricted_dataset.close(state=state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that in this case only the features are returned by `get_data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration schemes: which examples to visit\n",
    "\n",
    "Encapsulating and accessing our data is good, but if we're to integrate it into a training loop, we need to be able to iterate over the data. For that, we need to decide *which* indices to request and in *which order*. This is accomplished via an `IterationScheme` subclass.\n",
    "\n",
    "At its most basic level, an iteration scheme is responsible, through its `get_request_iterator` method, for building an iterator that will return requests. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.schemes import (SequentialScheme, ShuffledScheme,\n",
    "                          SequentialExampleScheme, ShuffledExampleScheme)\n",
    "\n",
    "schemes = [SequentialScheme(examples=8, batch_size=4),\n",
    "           ShuffledScheme(examples=8, batch_size=4),\n",
    "           SequentialExampleScheme(examples=8),\n",
    "           ShuffledExampleScheme(examples=8)]\n",
    "for scheme in schemes:\n",
    "    print([request for request in scheme.get_request_iterator()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can therefore use an iteration scheme to visit a dataset in some order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state = dataset.open()\n",
    "scheme = ShuffledScheme(examples=dataset.num_examples, batch_size=4)\n",
    "for request in scheme.get_request_iterator():\n",
    "    data = dataset.get_data(state=state, request=request)\n",
    "    print(data[0].shape, data[1].shape)\n",
    "dataset.close(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data streams: automating the iteration process\n",
    "\n",
    "Iteration schemes offer a more convenient way to visit the dataset than accessing the data by hand, but we can do better: the act of getting a fresh state from the dataset, getting a request iterator from the iteration scheme, using both to access the data and closing the state is repetitive. To automate this, we have *data streams*, which are subclasses of ``AbstractDataStream``.\n",
    "\n",
    "The most common ``AbstractDataStream`` subclass is `DataStream`. It is instantiated with a dataset and an iteration scheme, and returns an epoch iterator through its `get_epoch_iterator` method, which iterates over the dataset in the order defined by the iteration scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.streams import DataStream\n",
    "\n",
    "data_stream = DataStream(dataset=dataset, iteration_scheme=scheme)\n",
    "for data in data_stream.get_epoch_iterator():\n",
    "    print(data[0].shape, data[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers: apply some transformation on the fly\n",
    "\n",
    "Some data streams take data streams as input. We call them *transformers*, and they enable us to build complex data preprocessing pipelines.\n",
    "\n",
    "Transformers are ``Transformer`` subclasses. Most of the the transformers you'll encounter are located in the ``fuel.transformers`` module. Here are some commonly used ones:\n",
    "\n",
    "* ``Flatten``: flattens the input into a matrix (for batch input) or a vector (for examplewise input).\n",
    "* ``ScaleAndShift``: scales and shifts the input by scalar quantities.\n",
    "* ``Cast``: casts the input into some data type.\n",
    "\n",
    "As an example, let's standardize the images we have by substracting their mean and dividing by their standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuel.transformers import ScaleAndShift\n",
    "# Note: ScaleAndShift applies (batch * scale) + shift, as\n",
    "# opposed to (batch + shift) * scale.\n",
    "scale = 1.0 / features.std()\n",
    "shift = - scale * features.mean()\n",
    "standardized_stream = ScaleAndShift(data_stream=data_stream,\n",
    "                                    scale=scale, shift=shift,\n",
    "                                    which_sources=('features',))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting data stream can be used to iterate over the dataset just like before, but this time features will be standardized on-the-fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for batch in standardized_stream.get_epoch_iterator():\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's imagine that for some reason (e.g. running Theano code on GPU) we **need** features to have a data type of `float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuel.transformers import Cast\n",
    "cast_standardized_stream = Cast(data_stream=standardized_stream,\n",
    "                                dtype='float32', which_sources=('features',))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Fuel makes it easy to chain transformations to form a preprocessing pipeline. The complete pipeline now looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_stream = Cast(\n",
    "    ScaleAndShift(\n",
    "        DataStream(\n",
    "            dataset=dataset, iteration_scheme=scheme),   \n",
    "        scale=scale, shift=shift, which_sources=('features',)),\n",
    "    dtype='float32', which_sources=('features',))\n",
    "for batch in data_stream.get_epoch_iterator():\n",
    "    print(batch)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going further\n",
    "\n",
    "You now know enough to find your way around Fuel. Let's cover some more advanced use cases.\n",
    "\n",
    "## Large datasets\n",
    "\n",
    "Sometimes, the dataset you're working on is too big to fit in memory. In that case, you'll want to use another common ``Dataset`` subclass, `H5PYDataset`.\n",
    "\n",
    "### H5PYDataset\n",
    "\n",
    "As the name implies, `H5PYDataset` is a dataset class that interfaces with HDF5 files using the `h5py` library.\n",
    "\n",
    "HDF5 is a wonderful storage format, as it is organizable and self-documentable. This allows us to make a basic set of assumptions about the structure of an HDF5 file which, if met, greatly simplify creating new datasets and interacting with them. We won't go through these assumptions right now, but if you're curious, the online documentation offers an in-depth tutorial on how to create new `H5PYDataset`-compatible files.\n",
    "\n",
    "Let's create new random data. This time, we'll pretend that we're given a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_image_features = rng.randint(256, size=(90, 3, 32, 32)).astype('uint8')\n",
    "train_vector_features = rng.normal(size=(90, 16))\n",
    "train_targets = rng.randint(10, size=(90, 1)).astype('uint8')\n",
    "\n",
    "test_image_features = rng.randint(256, size=(10, 3, 32, 32)).astype('uint8')\n",
    "test_vector_features = rng.normal(size=(10, 16))\n",
    "test_targets = rng.randint(10, size=(10, 1)).astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create an HDF5 file and populate it with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "from fuel.converters.base import fill_hdf5_file\n",
    "f = h5py.File('dataset.hdf5', mode='w')\n",
    "data = (('train', 'image_features', train_image_features),\n",
    "        ('train', 'vector_features', train_vector_features),\n",
    "        ('train', 'targets', train_targets),\n",
    "        ('test', 'image_features', test_image_features),\n",
    "        ('test', 'vector_features', test_vector_features),\n",
    "        ('test', 'targets', test_targets))\n",
    "fill_hdf5_file(f, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fill_hdf5_file` function fills the HDF5 file with our data and sets up metadata so `H5PYDataset` is able to recover our train and test splits.\n",
    "\n",
    "Before closing the file, let's also tag axes with their label. The populated HDF5 file features one dataset per data source (in our case, `image_features`, `vector_features` and `targets`), whose dimensions we can tag with a name. `H5PYDataset` is able to recover this information and create an `axis_labels` dict for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, label in enumerate(('batch', 'channel', 'height', 'width')):\n",
    "    f['image_features'].dims[i].label = label\n",
    "for i, label in enumerate(('batch', 'feature')):\n",
    "    f['vector_features'].dims[i].label = label\n",
    "for i, label in enumerate(('batch', 'index')):\n",
    "    f['targets'].dims[i].label = label\n",
    "f.flush()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything we need to load this HDF5 file in Fuel.\n",
    "\n",
    "We'll instantiate `H5PYDataset` by passing it the path to our HDF5 file as well as a tuple of splits to use. For now, we'll just load the train and test sets separately, but note that it is also possible to concatenate splits that way (e.g. concatenate the training and validation sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuel.datasets import H5PYDataset\n",
    "train_dataset = H5PYDataset('dataset.hdf5', which_sets=('train',))\n",
    "test_dataset = H5PYDataset('dataset.hdf5', which_sets=('test',))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`H5PYDataset` instances allow the same level of introspection as `IndexableDataset` instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Sources are {}.'.format(train_dataset.sources))\n",
    "print('Axis labels are {}.'.format(train_dataset.axis_labels))\n",
    "print('Training set contains {} examples.'.format(train_dataset.num_examples))\n",
    "print('Test set contains {} examples.'.format(test_dataset.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate over data the same way as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_stream = DataStream(\n",
    "    dataset=train_dataset,\n",
    "    iteration_scheme=ShuffledScheme(\n",
    "        examples=train_dataset.num_examples, batch_size=10))\n",
    "for batch in train_stream.get_epoch_iterator():\n",
    "    print([source.shape for source in batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H5PYDataset for small datasets\n",
    "\n",
    "The `H5PYDataset` class isn't suitable only to large datasets. In fact, most of Fuel's built-in datasets rely on  HDF5 for storage.\n",
    "\n",
    "At first sight, this might seem inefficient (data in an HDF5 file is read off disk instead of being stored in memory, which is considerably slower), but `H5PYDataset` features a `load_in_memory` constructor argument which, when set to `True`, reads data off disk once and stores it in memory as a numpy array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in datasets\n",
    "\n",
    "Fuel aims to facilitate iterating over and transforming data, which we've covered up to now, but one of its goals is also to make it easy to download, convert and store often-used datasets. This is what will be covered in this section.\n",
    "\n",
    "Built-in datasets are datasets which can be obtained through Fuel's automated downloading and conversion tools. Here are some built-in datasets available in Fuel:\n",
    "\n",
    "* Iris\n",
    "* MNIST\n",
    "* Binarized MNIST\n",
    "* CIFAR10\n",
    "* CIFAR100\n",
    "* SVHN (format 1, format 2)\n",
    "* Caltech 101 silhouettes\n",
    "\n",
    "### Defining where Fuel looks for data\n",
    "\n",
    "Fuel implements specific ``Dataset`` subclasses for each of the built-in datasets. They all expect their corresponding data files to be contained inside one of the directories defined in the Fuel data path.\n",
    "\n",
    "You can define this data path by setting the `data_path` variable in `~/.fuelrc`:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ~/.fuelrc\n",
    "\n",
    "data_path: /home/user/data_location_1:/home/user/data_location_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can override it by setting the `FUEL_DATA_PATH` environment variable.\n",
    "\n",
    "In both cases, Fuel expects a sequence of paths separated by an OS-specific delimiter (`:` for Linux / Mac OS, `;` for Windows).\n",
    "\n",
    "Let's create a directory in which to put our data files and set it as our Fuel data path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir fuel_data\n",
    "import os\n",
    "os.environ['FUEL_DATA_PATH'] = os.path.abspath('./fuel_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading raw data files\n",
    "\n",
    "Fuel comes with a `fuel-download` script which automates downloading raw data files for built-in datasets. You can have a look at the full list of built-in datasets with `fuel-download -h`. For now, we'll download the MNIST dataset in our newly-created data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!fuel-download mnist -d $FUEL_DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting raw data files\n",
    "\n",
    "We'll now convert the raw data files to an HDF5 file which the `MNIST` dataset class can read. This is done using the `fuel-convert` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!fuel-convert mnist -d $FUEL_DATA_PATH -o $FUEL_DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using built-in datasets\n",
    "\n",
    "Now that the data has been downloaded and converted, we can instantiate and use the built-in dataset class just like any other `H5PYDataset` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets import MNIST\n",
    "from matplotlib import pyplot, cm\n",
    "\n",
    "dataset = MNIST(('train',), sources=('features',))\n",
    "state = dataset.open()\n",
    "image, = dataset.get_data(state=state, request=[1234])\n",
    "pyplot.imshow(image.reshape((28, 28)), cmap=cm.Greys_r, interpolation='nearest')\n",
    "pyplot.show()\n",
    "dataset.close(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default transformers\n",
    "\n",
    "Datasets can define a convenience transformer pipeline, which can automatically be applied when instantiating a data stream by using the alternative ``DataStream.default_stream`` constructor. We call these *default transformers*. Use cases for default transformers include the following:\n",
    "\n",
    "* To save disk space, some datasets may store their data in a format that's different from the format that's typically used for machine learning applications. This is the case for the MNIST, CIFAR10 and CIFAR100 built-in datasets: the raw data being 8-bit images, the datasets are stored using `uint8` bytes, which is space-efficient. However, this means that pixel values range from 0 to 255, as opposed to the `[0.0, 1.0]` range machine learning practitioners are used to. In order to reduce the amount of boilerplate code users have to write to use these datasets, their default transformers divide features by 255 and cast them as `floatX`.\n",
    "* Some datasets, such as SVHN or ImageNet, are composed of variable-length features, but some preprocessing (e.g. scale the short side of the image to 256 pixels and take a random square crop) is usually applied to obtain fixed-sized features. Although there is no unique way to preprocess these features, such datasets may define default transformers corresponding to an often-used method, or to a method used in a landmark paper (e.g. AlexNet).\n",
    "\n",
    "Default transformers are defined through the `default_transformers` class attribute. It is expected to be a tuple with one element per transformer in the pipeline. Each element is a tuple with three elements:\n",
    "\n",
    "* the ``Transformer`` subclass to apply,\n",
    "* a list of arguments to pass to the subclass constructor, and\n",
    "* a dict of keyword arguments to pass to the subclass constructor.\n",
    "\n",
    "Let's look at what MNIST defines as a default transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(MNIST.default_transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like explained above, MNIST defines a two-transformer pipeline as its default transformers. The first transformer scales the features by 1 / 255 so that they range between 0 and 1, and the second transformer casts the features to ``floatX``.\n",
    "\n",
    "Let's compare the output of a data stream with and without the default transformers applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vanilla_stream = DataStream(\n",
    "    dataset=dataset,\n",
    "    iteration_scheme=SequentialExampleScheme(dataset.num_examples))\n",
    "print(next(vanilla_stream.get_epoch_iterator())[0].max())\n",
    "\n",
    "default_stream = DataStream.default_stream(\n",
    "    dataset=dataset,\n",
    "    iteration_scheme=SequentialExampleScheme(dataset.num_examples))\n",
    "print(next(default_stream.get_epoch_iterator())[0].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending Fuel\n",
    "\n",
    "### New dataset classes\n",
    "\n",
    "New dataset classes are implemented by subclassing `Dataset` and implementing a `get_data` method. If your dataset interacts with stateful objects (e.g. files on disk), then you should also override the `open` and `close` methods.\n",
    "\n",
    "If your data fits in memory, you can save yourself some time by inheriting from `IndexableDataset`. In that case, all you need to do is load the data as a `dict` mapping source names to their corresponding data and pass it to the superclass as the `indexables` argument.\n",
    "\n",
    "For instance, here's how you would implement a specialized class to interface with `.npy` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from six import iteritems\n",
    "\n",
    "\n",
    "class NPYDataset(IndexableDataset):\n",
    "    def __init__(self, source_paths, **kwargs):\n",
    "        indexables = dict(\n",
    "            [(source, numpy.load(path)) for\n",
    "             source, path in iteritems(source_paths)])\n",
    "        super(NPYDataset, self).__init__(indexables, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's this class in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numpy.save('fuel_data/npy_dataset_features.npy',\n",
    "           numpy.arange(40).reshape((10, 4)))\n",
    "numpy.save('fuel_data/npy_dataset_targets.npy',\n",
    "           numpy.arange(10).reshape((10, 1)))\n",
    "dataset = NPYDataset({'features': 'fuel_data/npy_dataset_features.npy',\n",
    "                      'targets': 'fuel_data/npy_dataset_targets.npy'})\n",
    "state = dataset.open()\n",
    "print(dataset.get_data(state=state, request=[0, 1, 2, 3]))\n",
    "dataset.close(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New transformers\n",
    "\n",
    "An important thing to know about data streams is that they distinguish between two types of outputs: single examples, and batches of examples. Depending on your choice of iteration scheme, a data stream's `produces_examples` property will either be `True` (it produces examples) or `False` (it produces batches).\n",
    "\n",
    "Transformers are aware of this, and as such implement two distinct methods: `transform_example` and `transform_batch`. A new transformer is typically implemented by subclassing `Transformer` and implementing one or both of these methods.\n",
    "\n",
    "As an example, here's how you would double the value of the `'features'` data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuel.transformers import Transformer\n",
    "\n",
    "\n",
    "class FeaturesDoubler(Transformer):\n",
    "    def __init__(self, data_stream, **kwargs):\n",
    "        super(FeaturesDoubler, self).__init__(\n",
    "            data_stream=data_stream,\n",
    "            produces_examples=data_stream.produces_examples,\n",
    "            **kwargs)\n",
    "        \n",
    "    def transform_example(self, example):\n",
    "        if 'features' in self.sources:\n",
    "            example = list(example)\n",
    "            index = self.sources.index('features')\n",
    "            example[index] *= 2\n",
    "            example = tuple(example)\n",
    "        return example\n",
    "    \n",
    "    def transform_batch(self, batch):\n",
    "        if 'features' in self.sources:\n",
    "            batch = list(batch)\n",
    "            index = self.sources.index('features')\n",
    "            batch[index] *= 2\n",
    "            batch = tuple(batch)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most transformers you'll implement will call their superclass constructor by passing the data stream and declaring whether they produce examples or batches. Since we wish to support both batches and examples, we'll declare our output type to be the same as our data stream's output type.\n",
    "\n",
    "If you were to build a transformer that only works on batches, you would pass `produces_examples=False` and implement only `transform_batch`. If anyone tried to use your transformer on an example data stream, an error would automatically be raised.\n",
    "\n",
    "Let's test our doubler on some dummy dataset. **Note that the this implementation is brittle and only works on numpy arrays.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = IndexableDataset(\n",
    "    indexables={'features': numpy.array([1, 2, 3, 4]),\n",
    "                'targets': numpy.array([-1, 1, -1, 1])})\n",
    "example_scheme = SequentialExampleScheme(examples=dataset.num_examples)\n",
    "example_stream = FeaturesDoubler(\n",
    "    data_stream=DataStream(\n",
    "        dataset=dataset, iteration_scheme=example_scheme))\n",
    "batch_scheme = SequentialScheme(\n",
    "    examples=dataset.num_examples, batch_size=2)\n",
    "batch_stream = FeaturesDoubler(\n",
    "    data_stream=DataStream(\n",
    "        dataset=dataset, iteration_scheme=batch_scheme))\n",
    "print([example for example in example_stream.get_epoch_iterator()])\n",
    "print([batch for batch in batch_stream.get_epoch_iterator()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you think the `transform_example` and `transform_batch` implementations are repetitive, you're right! In cases where the example and batch implementations of a transformer are the same, you can subclass from `AgnosticTransformer` instead. It requires that you implement a `transform_any` method, which will be called by both `transform_example` and `transform_batch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuel.transformers import AgnosticTransformer\n",
    "\n",
    "class FeaturesDoubler(AgnosticTransformer):\n",
    "    def __init__(self, data_stream, **kwargs):\n",
    "        super(FeaturesDoubler, self).__init__(\n",
    "            data_stream=data_stream,\n",
    "            produces_examples=data_stream.produces_examples,\n",
    "            **kwargs)\n",
    "\n",
    "    def transform_any(self, data):\n",
    "        if 'features' in self.sources:\n",
    "            data = list(data)\n",
    "            index = self.sources.index('features')\n",
    "            data[index] *= 2\n",
    "            data = tuple(data)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our transformer could be more general: what if we want to double `'features'` *and* `'targets'`, or only `'targets'`? \n",
    "\n",
    "Transformers which are applied sourcewise like our doubler should usually subclass from `SourcewiseTransformer`. Their constructor takes an additional `which_sources` keyword argument specifying which sources to apply the transformer to. It's expected to be a tuple of source names. If `which_sources` is `None`, then the transformer is applied to *all* sources. Subclasses of `SourcewiseTransformer` should implement a `transform_source_example` method and/or a `transform_source_batch` method, which apply on an individual source.\n",
    "\n",
    "There also exists an `AgnosticSourcewiseTransformer` class for cases where the example and batch implementations of a sourcewise transformer are the same. This class requires a `transform_any_source` method to be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuel.transformers import AgnosticSourcewiseTransformer\n",
    "\n",
    "class Doubler(AgnosticSourcewiseTransformer):\n",
    "    def __init__(self, data_stream, **kwargs):\n",
    "        super(Doubler, self).__init__(\n",
    "            data_stream=data_stream,\n",
    "            produces_examples=data_stream.produces_examples,\n",
    "            **kwargs)\n",
    "\n",
    "    def transform_any_source(self, source, _):\n",
    "        return 2 * source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this implementation on our dummy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_stream = Doubler(\n",
    "    data_stream=DataStream(\n",
    "        dataset=dataset,\n",
    "        iteration_scheme=batch_scheme),\n",
    "    which_sources=('targets',))\n",
    "\n",
    "all_stream = Doubler(\n",
    "    data_stream=DataStream(\n",
    "        dataset=dataset,\n",
    "        iteration_scheme=batch_scheme),\n",
    "    which_sources=None)\n",
    "\n",
    "print([batch for batch in target_stream.get_epoch_iterator()])\n",
    "print([batch for batch in all_stream.get_epoch_iterator()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, there exists a `Mapping` transformer which acts as a swiss-army knife transformer. In addition to a data stream, its constructor accepts a function which will be applied to data coming from the stream.\n",
    "\n",
    "Here's how you would implement the feature doubler using `Mapping`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.transformers import Mapping\n",
    "\n",
    "features_index = dataset.sources.index('features')\n",
    "def double(data):\n",
    "    data = list(data)\n",
    "    data[features_index] *= 2\n",
    "    return tuple(data)\n",
    "\n",
    "mapping_stream = Mapping(\n",
    "    data_stream=DataStream(\n",
    "        dataset=dataset, iteration_scheme=batch_scheme),\n",
    "    mapping=double)\n",
    "\n",
    "print([batch for batch in mapping_stream.get_epoch_iterator()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New iteration schemes\n",
    "\n",
    "New iteration schemes are implemented by subclassing `IterationScheme` and implementing a `get_request_iterator` method, which should return an iterator that returns lists of indices.\n",
    "\n",
    "Two subclasses of `IterationScheme` typically serve as a basis for other iteration schemes: `IndexScheme` (for schemes requesting examples) and `BatchScheme` (for schemes requesting batches). Both subclasses are instantiated by providing a list of indices or a number of examples, and `BatchScheme` accepts an additional `batch_size` argument.\n",
    "\n",
    "Here's how you would implement an iteration scheme that iterates over even examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuel.schemes import IndexScheme, BatchScheme\n",
    "# `iter_` : A picklable version of `iter`\n",
    "from picklable_itertools import iter_, imap\n",
    "# Partition all elements of a sequence into tuples of length at most n\n",
    "from picklable_itertools.extras import partition_all\n",
    "\n",
    "\n",
    "class ExampleEvenScheme(IndexScheme):\n",
    "    def get_request_iterator(self):\n",
    "        indices = list(self.indices)[::2]\n",
    "        return iter_(indices)\n",
    "\n",
    "\n",
    "class BatchEvenScheme(BatchScheme):\n",
    "    def get_request_iterator(self):\n",
    "        indices = list(self.indices)[::2]\n",
    "        return imap(list, partition_all(self.batch_size, indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the two iteration scheme classes in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(list(ExampleEvenScheme(10).get_request_iterator()))\n",
    "print(list(BatchEvenScheme(10, 2).get_request_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelizing data processing\n",
    "\n",
    "Fuel allows to parallelize data processing in a separate process. This feature is still under development, but it is already pretty useful.\n",
    "\n",
    "Implementing a parallelized preprocessing pipeline is done in two steps. At first, you should write a Python script that sets up the data processing pipeline and spawns a server that listens to requests. See the `fuel_server` notebook for more details on that.\n",
    "\n",
    "Once the server is up and running, you'll need to instantiate a `ServerDataStream` instance, which will connect to the server and make requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "\n",
    "from fuel.streams import DataStream, ServerDataStream\n",
    "from fuel.transformers import Transformer\n",
    "\n",
    "\n",
    "class Bottleneck(Transformer):\n",
    "    def __init__(self, data_stream, **kwargs):\n",
    "        self.slowdown = kwargs.pop('slowdown', 0)\n",
    "        super(Bottleneck, self).__init__(\n",
    "            data_stream, data_stream.produces_examples, **kwargs)\n",
    "\n",
    "    def get_data(self, request=None):\n",
    "        if request is not None:\n",
    "            raise ValueError\n",
    "        time.sleep(self.slowdown)\n",
    "        return next(self.child_epoch_iterator)\n",
    "\n",
    "\n",
    "dataset = IndexableDataset({'features': [[0] * 128] * 1000})\n",
    "iteration_scheme = ShuffledScheme(examples=1000, batch_size=100)\n",
    "regular_data_stream = Bottleneck(\n",
    "    data_stream=DataStream(\n",
    "        dataset=dataset, iteration_scheme=iteration_scheme),\n",
    "    slowdown=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def time_iteration(parallel):\n",
    "    if parallel:\n",
    "        data_stream = ServerDataStream(('features',), produces_examples=False)\n",
    "    else:\n",
    "        data_stream = regular_data_stream\n",
    "\n",
    "    start_time = time.time()\n",
    "    for i in range(10):\n",
    "        for data in data_stream.get_epoch_iterator(): time.sleep(0.01)\n",
    "    stop_time = time.time()\n",
    "    print('Training took {} seconds'.format(stop_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_iteration(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_iteration(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
